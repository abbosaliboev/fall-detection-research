# Fall-detection-research
A Research project focused on detecting human falls using computer vision and machine learning techniques for improved safety in smart environments.



## A spatio-temporal graph neural network for fall prediction with inertial sensors
https://www.sciencedirect.com/science/article/abs/pii/S0950705124003447
In this paper, we present a GNN-based fall prediction method, which can capture both temporal and spatial relations among poses, especially for the Falling phase in complex scenes. It is more efficient than existing methods in fall prediction, particularly in reducing the miss or false alarm rates for fall events. As for future work, we will consider learning causal relations among poses for robustness and exploring the applications of our model on other phases in fall events.

## A real-time system for fall prediction and protection with spatio-temporal graph neural network using multiple motion sensors
https://www.sciencedirect.com/science/article/abs/pii/S0957417425011716
In this paper, we propose a practical real-time fall prediction system designed to mitigate injuries caused by human falls. We introduce a neural network model based on spatio-temporal dynamic graphs, which successfully captures the spatio-temporal features of motion signals moments before a fall from multiple motion sensors. This model facilitates the dynamic topological reconstruction of human motion information, enabling the system to rapidly and accurately identify impending falls. As a

## Multimodal fall detection for solitary individuals based on audio-video decision fusion processing
https://www.sciencedirect.com/science/article/pii/S2405844024056275
Falls are a major safety concern in single-person households, and older adults are particularly vulnerable. Consequently, the development of precise and efficient fall detection methodologies has emerged as a focal point of contemporary research. Furthermore, the integration of multimodal technologies is anticipated to substantially enhance detection accuracy and robustness by leveraging the complementary strengths of diverse modalities.
In this study, we explore ways to combine video and audio modalities based on non-invasive devices to enhance fall detection. Specifically, we employ the YOLOv7-Pose algorithm to identify critical skeletal nodes in video data, while two stream ST-GCN extracts pertinent spatial and temporal information to render a frame-level decision. This decision is then aggregated to formulate a video-level conclusion. Concurrently, in the audio domain, we implement a data augmentation technique to enrich our dataset, followed by the extraction of features using the log-scaled mel spectrogram. Subsequent analysis employs k-fold cross-validation to contrast the performance metrics of the MobileNetV2 and ResNet18 models. Despite the similar accuracy of the two architectures, MobileNetV2 is ultimately selected for its superior computational efficiency and more compact model size.
In order to test the generalization ability of the model, the original dataset is subjected to a variety of transformations including dimming the brightness, mirror inversion, changing the time domain, changing the volume and adding noise. The subsequent analysis reveals that the video modality struggles to accurately detect falls under conditions of low illumination or significant obstruction of the human figure. The efficacy of the inference results is significantly enhanced through the application of decision-level fusion techniques combining audio modalities, with optimal outcomes observed when the video modality is assigned a weight of 0.7. Furthermore, it is determined that decision fusion employing D-S theory yields a more balanced performance in the model compared to linear weighting methods.
We believe that multimodal audio-visual fusion methods, due to their advantages, are expected to become the optimal means of fall detection. In future research, we expect to apply our work to implement automatic fall detection systems in actual surveillance camera networks; and further optimize and improve the model based on actual data, and consider integrating more thorough feature-level fusion technology, paying special attention to time alignment methods; additionally there will be an emphasis on addressing privacy and data security issues, including the incorporation of image and audio anonymization techniques.


## Human Fall Detection using YOLO: A Real-Time and AI-on-the-Edge Perspective
https://ieeexplore.ieee.org/document/9854070
In this paper, a computer vision-based fall detection system has been proposed using YOLO and its variants. The research concluded that YOLO variants are effective for detecting human falls on edge devices in real-time. We have improved the accuracy of real-time human fall detection by using the YOlO algorithm and its variants for the UR fall dataset. As a result of this study, Tiny-YOLOV4 was identified as the most appropriate model for real-time fall detection. This same principle can be further extended to human activity recog-nition. In the future, we can extend this study by exploring alternative deep learning models for human fall detection and conducting an experimental study on human fall detection using different techniques and datasets. We are also exploring the use of pose estimation techniques to develop a real-time fall detection system. For pose estimation as a backbone, we intend to use different pose extractors, alphapose, blazepose, and openpose, to classify whether fall occurs or not. For fall detection, we also use vision transformers that employ attention mechanisms.
